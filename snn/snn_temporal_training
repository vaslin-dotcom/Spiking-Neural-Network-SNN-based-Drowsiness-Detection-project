from __future__ import print_function
import cv2
import torchvision
import numpy as np
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import time
import torch.nn.functional as F
import dlib
# Replace 'cv2_imshow(image)' with:
from google.colab.patches import cv2_imshow  # Import cv2_imshow for Colab


# Set up device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters and constants
thresh = 0.5    # Neuronal threshold
lens = 0.5      # Hyper-parameter of the approximate function
decay = 0.2     # Decay constant
num_classes = 10
batch_size = 100
learning_rate = 1e-3
num_epochs = 60
time_window = 22  # Simulation time steps

# Define the approximate firing function
class ActFun(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.gt(thresh).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        temp = abs(input - thresh) < lens
        return grad_input * temp.float()

act_fun = ActFun.apply

# Membrane potential update
def mem_update(ops, x, mem, spike):
    mem = mem * decay * (1. - spike) + ops(x)
    spike = act_fun(mem)  # Apply firing function
    return mem, spike

# CNN layer configuration (64x64 input)
cfg_cnn = [(1, 32, 1, 1, 3),
           (32, 32, 1, 1, 3)]
cfg_kernel = [28,14,7]  # Updated for 64x64 input
cfg_fc = [128, 2]

# Define learning rate scheduler
def lr_scheduler(optimizer, epoch, init_lr=0.1, lr_decay_epoch=50):
    if epoch % lr_decay_epoch == 0 and epoch > 1:
        for param_group in optimizer.param_groups:
            param_group['lr'] = param_group['lr'] * 0.1
    return optimizer

# Define Spiking CNN
class SCNN(nn.Module):
    def __init__(self):
        super(SCNN, self).__init__()
        in_planes, out_planes, stride, padding, kernel_size = cfg_cnn[0]
        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)
        in_planes, out_planes, stride, padding, kernel_size = cfg_cnn[1]
        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)

        # Fully connected layers
        self.fc1 = nn.Linear(cfg_kernel[-1] * cfg_kernel[-1] * cfg_cnn[-1][1], cfg_fc[0])
        self.fc2 = nn.Linear(cfg_fc[0], cfg_fc[1])

    def forward(self, input, time_window=time_window):
        batch_size = input.size(0)
        c1_mem = c1_spike = torch.zeros(batch_size, cfg_cnn[0][1], cfg_kernel[0], cfg_kernel[0], device=device)
        c2_mem = c2_spike = torch.zeros(batch_size, cfg_cnn[1][1], cfg_kernel[1], cfg_kernel[1], device=device)
        h1_mem = h1_spike = h1_sumspike = torch.zeros(batch_size, cfg_fc[0], device=device)
        h2_mem = h2_spike = h2_sumspike = torch.zeros(batch_size, cfg_fc[1], device=device)

        # Compute spike times based on input intensity
        spike_times = (1.0 - input) * time_window  # Scale input to spike times

        for step in range(time_window):
            # Latency encoding: Generate spikes based on spike times
            x = (spike_times <= step).float()  # Spike if current step >= spike time

            # First convolutional layer
            c1_mem, c1_spike = mem_update(self.conv1, x.float(), c1_mem, c1_spike)
            x = F.avg_pool2d(c1_spike, 2)

            # Second convolutional layer
            c2_mem, c2_spike = mem_update(self.conv2, x, c2_mem, c2_spike)
            x = F.avg_pool2d(c2_spike, 2)

            # Flatten and pass through fully connected layers
            x = x.view(batch_size, -1)
            h1_mem, h1_spike = mem_update(self.fc1, x, h1_mem, h1_spike)
            h1_sumspike += h1_spike
            h2_mem, h2_spike = mem_update(self.fc2, h1_spike, h2_mem, h2_spike)
            h2_sumspike += h2_spike

        # Average spikes over the time window
        outputs = h2_sumspike / time_window
        return outputs




# Variables
your_input_size = 28  # Adjust based on your model's input size

# Load pre-trained SNN model and face/landmark detector
snn = torch.load(r'/content/drive/MyDrive/DROWSINESS_DETECTION/eye_detection_snn_model_10.pth', map_location=torch.device('cpu'))  # Update with your SNN model path
snn.eval()  # Set the model to evaluation mode
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(r'/content/drive/MyDrive/DROWSINESS_DETECTION/shape_predictor_68_face_landmarks.dat')

# Load the image
image_path = r'/content/drive/MyDrive/DROWSINESS_DETECTION/image.jpg'  # Replace with the path to your image
image = cv2.imread(image_path)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Detect faces in the image
faces = detector(gray)

# Process each face detected in the image
for face in faces:
    # Get the landmarks
    landmarks = predictor(gray, face)

    # Extract the eye landmarks (36-41 for left eye, 42-47 for right eye)
    left_eye_indices = [36, 37, 38, 39, 40, 41]  # Left eye landmarks
    right_eye_indices = [42, 43, 44, 45, 46, 47]  # Right eye landmarks

    # Get the coordinates for the left and right eyes
    left_eye_points = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in left_eye_indices])
    right_eye_points = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in right_eye_indices])

    # Get the bounding box of the left and right eyes
    x_left, y_left, w_left, h_left = cv2.boundingRect(left_eye_points)
    x_right, y_right, w_right, h_right = cv2.boundingRect(right_eye_points)

    # Crop the eye region for the left and right eye
    left_eye_roi = gray[y_left:y_left + h_left, x_left:x_left + w_left]
    right_eye_roi = gray[y_right:y_right + h_right, x_right:x_right + w_right]

    # Resize for the model and normalize (scale to [0, 1])
    left_eye_roi_resized = cv2.resize(left_eye_roi, (your_input_size, your_input_size), interpolation=cv2.INTER_AREA) / 255.0
    right_eye_roi_resized = cv2.resize(right_eye_roi, (your_input_size, your_input_size), interpolation=cv2.INTER_AREA) / 255.0

    # Reshape for the model (convert to tensor)
    left_eye_tensor = torch.tensor(left_eye_roi_resized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch dimension
    right_eye_tensor = torch.tensor(right_eye_roi_resized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch dimension

    # Forward pass through the SNN model
    with torch.no_grad():
        left_output = snn(left_eye_tensor)
        right_output = snn(right_eye_tensor)

    # Assuming the output is a single value representing the probability of being closed
    left_label = 'Closed' if torch.argmax(left_output) == 0 else 'Opened'
    right_label = 'Closed' if torch.argmax(right_output) == 0 else 'Opened'

    # Print the results
    print(f"Left eye is {left_label}")
    print(f"Right eye is {right_label}")

    # Display the image with eye bounding boxes and labels
    cv2.rectangle(image, (x_left, y_left), (x_left + w_left, y_left + h_left), (255, 0, 0), 2)
    cv2.putText(image, left_label, (x_left - 10, y_left - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
    cv2.rectangle(image, (x_right, y_right), (x_right + w_right, y_right + h_right), (255, 0, 0), 2)
    cv2.putText(image, right_label, (x_right - 10, y_right - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

# Show the image with results
cv2_imshow( image)
cv2.waitKey(0)
cv2.destroyAllWindows()
